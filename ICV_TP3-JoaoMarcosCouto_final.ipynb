{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre a escolha de arquiteturas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apos ler um pouco sobre possiveis metodologias de selecao de arquiteturas de redes neurais, prontamente percebi a existencia de um consenso de que a definicao de arquiteturas eh um processo bastante empirico e que varia bastante de acordo com a complexidade da distribuicao que impera sobre cada base de dados. Assim, nao existe uma unica arquitetura que vai funcionar para todos os datasets.\n",
    "\n",
    "Todavia alguns em minhas leituras indentifiquei algumams intuicoes sao validas como 'rules-of-thumb' para um largo espectro de problemas.Dentre essas intuicoes escolhi algumas que me guiaram na elaboracao das arquitetures que utilizei neste TP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuicoes e rules-of-thumb escolhidos: DropOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira delas, eh que o processo de treinamento de redes neurais sofre cronicamente com problemas de overffiting. Neuronios com muita facilidade acabam criando relacoes muito fortes com alguns poucos neuronios especificos da camada anterior a eles, muitas vezes comprometendo severam a capacidade de generalizacao da rede\n",
    "\n",
    "Nesse contexto a nocao de aleatoriamente desligar um parcela de neuronios da camada anterior durante o processo de treinamento forca a rede a aprender representacoes mais generalizaveis do dado de entrada... Ou seja, que sao mais uteis para um espectro maior de entrada e nao apenas algumas entradas especificadas da base de treinamento\n",
    "\n",
    "Assim, o Dropout eh uma ferramente extramemente poderosa para os tipos de rede neural que estamos utilizando neste TP portanto pode e deve ser utilizado com frequencia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuicoes e rules-of-thumb escolhidos: padroes dos tamanhos de camadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O segundo principal elemento que frequentemente apareceu em minhas pesquisas por boas arquiteturas surgiu na forma da progressao dos tamanhos das camadas tanto das redes neurais quanto nas redes neurais convolucionais\n",
    "\n",
    "Observei que muitas vezes opta-se por fazer um progressao em potencias de 2 quando estamos decidindo tanto o numero de neuronios em uma camada de NN's quanto o numero de filtros em uma CNN. Nao encontrei respostas conclusivas sobre o porque isso eh feito todavia desenvolvi duas intuicoes que me ajudaram a acreditar nesta pratica comum:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuicoes e rules-of-thumb escolhidos: progressao no numero de filtros em RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso das RNNS, observei que as progressoes no numero de filtros a medida que fazemos mais convolucoes tende a aumentar em potencias de dois, tal como 8 -> 16 -> 32. Acredito que a intuicao que justifica essa pratica eh que em cada camada convolucional os filtros estao la para tentar capturar padroes na imagem.. Entao podemos imaginar que os primeiros filtros capturam padroes mais simples como quinas e linha verticais/horizontais.. Com esses padroes em maos queremos entao combina-los em padroes maiores e mais complexos.. Assim como os padroes ficam mais e mais compleos a medida que avancamos nas camadas convolucionais, temos cada vez mais possiveis padroes que podem ser formados.. Assim, progressivamente aumentamos o numero de filtros: para capturar a maior quantidade de combinacoes uteis possiveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuicoes e rules-of-thumb escolhidos: progressao no numero de neuronios em NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso das NNs, observei que as progressoes geralmente diminuem em potencias de dois tal como 1024 -> 512 ->256..\n",
    "Acredito que a motivacao por tras disso eh que queremos que rede neural desenvolva novas representacoes cada vez mais elaboradas e compostas do dado rebido na entrada.. Assim, embora tenhamos recebido 1024 features de entrada, eh interessante para a classificao final que a rede neural extraia representacoes cada vez mais poderosas para o dado.. Para tal, ao forcamos a reducao no numero de neuronios, forcamos que a rede aprenda nocoes que contenham uma densidade cada vez maior de representacoes uteis para a classificacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juntando tudo: meu bloco denso e meu bloco convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo absorvido essas intuicoes, criei um bloco denso padrao e um bloco convolucional padrao que entao posso emilhar multiplas vezes sequencialmente na hora de montar minhas redes\n",
    "\n",
    "Bloco convolucional: Conv2D com x filtros -> Dropout -> Conv2D com x/2 filtros -> Maxpooling2D\n",
    "Bloco denso: Dropout -> Dense com x neuronios\n",
    "\n",
    "Assim minha arquitetura NN tornou-se uma conjuncao de multiplos desse bloco denso e minha arquitetura CNN tornou-se uma conjuncao de multiplos desse bloco convolucional (para extrair features) seguidos de multiplos do bloco denso para fazer a classificao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como escolhi o numero de blocos a utilizar e os hiperparametros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiz uma mini verso do que eu entendi ser o tal *grid search*! \n",
    "\n",
    "Para cada um dos hiperparametros que surgem das intuicoes que explicitei acima, fiz um grid search utilizando bons candidatos para cada parametro\n",
    "\n",
    "Em outras palavras, treinei uma nova rede para cada possivel combinacao dos hiperparametros relevantes\n",
    "\n",
    "Ao final, os seguintes hiperpametros foram decididos dessa forma:\n",
    "\n",
    "    O numero de blocos densos na minha rede NN\n",
    "    \n",
    "    O numero de blocos densos na minha rede CNN\n",
    "    \n",
    "    O numero de blocos convolucionais na minha rede CNN\n",
    "    \n",
    "    O learning rate tanto da minha CNN quanto da minha NN\n",
    "    \n",
    "    O dropout rate tanto da minha CNN quando da minha NN\n",
    "    \n",
    "    \n",
    "Assim ao final treinei \n",
    "\n",
    "4 redes neurais variando o numero de blocos densos\n",
    "4 redes neurais variando o learning rate e o dropout rate\n",
    "\n",
    "4 redes neurais convolucionais variando o numero de blocos densos e o numero de blocos convolucionais\n",
    "4 redes neurais convolucionais variando o learning rate e o dropout rate\n",
    "\n",
    "Ao final utilizei a acuracia no conjunto de validacao para escolhar qual a melhor RNN e qual a melhor NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre outros hiperparametros:\n",
    "\n",
    "Professor, como pode ver acima optei por fazer um grid search extenso (para um computador pessoal..) nos hiperparametros das intuicoes que escolhi como guias na hora de escolher a arquitetura da minha rede..\n",
    "\n",
    "Entao por motivos de complexidade computacional, outros parametros como batch_size e numero de epocas foram escolhidos de forma puramente empirica em redes que treinei manualmente..\n",
    "\n",
    "Espero que entenda que eu poderia muito bem ter feito o mesmo processo de grid search com eles.. Mas inncluir esses parametros a mais no meu grid search quadrupilacaria meu tempo de treinamento, que ja foi muito longo.. Isso tornaria meu tempo de iteracao em modificacoes no meu TP demasiadamente longo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D, AveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.metrics import AUC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    \n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    \n",
    "    ax.set_xticklabels(classes)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    plt.xlim(-0.5, len(np.unique(y_true))-0.5)\n",
    "    plt.ylim(len(np.unique(y_true))-0.5, -0.5)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            #ax.text(j, i, format(cm[i, j], fmt),\n",
    "            ax.text(j, i, format(annot[i, j]),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from matplotlib import pyplot\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Normalização das features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### One-hot dos labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Criando gerador de data augmentation para uso na LeNet e na CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_LeNet = Sequential()\n",
    "#A dimensao do output da Cond2D eh (dimensao original - dimensao do filtro mais + 1)\n",
    "#Assim como queremos output 28x28 para entrada 32x32 utilizados filtro 5x5 \n",
    "model_LeNet.add(Conv2D(6, kernel_size=(5,5), padding='valid', activation='tanh', input_shape=(32, 32, 3)))\n",
    "#Agora pela modelagem original da LeNet 5 queremos um pooling que resulte em 14x14 saindo de 28x28 logo usamos stride 2\n",
    "model_LeNet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "#Mesma logica do primmeiro Conv2D, so que queremos output 10x10 para 16 filtros\n",
    "model_LeNet.add(Conv2D(16, kernel_size=(5, 5), activation='tanh', padding='valid'))\n",
    "#Novamente queremos reduzir a dimensao pela metade na saida do pooling\n",
    "model_LeNet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "model_LeNet.add(Flatten())\n",
    "model_LeNet.add(Dense(120, activation='tanh'))\n",
    "model_LeNet.add(Dense(84, activation='tanh'))\n",
    "model_LeNet.add(Dense(10, activation='softmax'))\n",
    "lrate = 0.001\n",
    "adam = Adam(learning_rate = lrate)\n",
    "model_LeNet.compile(loss='categorical_crossentropy', optimizer= adam, metrics=['accuracy']) \n",
    "#print(model_LeNet.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "aug_train = datagen.flow(X_train, y_train, batch_size= batch_size)\n",
    "steps = int(X_train.shape[0] / batch_size )\n",
    "history_LeNet = model_LeNet.fit_generator(aug_train, steps_per_epoch=steps, epochs=epochs, validation_data=(X_test, y_test))\n",
    "# Final evaluation of the model\n",
    "scores = model_LeNet.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de confusao LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model_LeNet.predict(X_test)\n",
    "max_index_row_pred = np.argmax(a, axis=1)\n",
    "max_index_row_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "label_names = ['airplane','automobile','bird','cat',  'deer','dog','frog','horse','ship', 'truck']\n",
    "\n",
    "plot_confusion_matrix(max_index_row_test, max_index_row_pred, \n",
    "                      #classes=['0' ,'1','2','5', '6', '7', '4', '8', '3','9'],\n",
    "                      classes= label_names,\n",
    "                      title='LeNet5 CIFAR-10 confusion matrix',\n",
    "                     cmap = plt.cm.Oranges\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(dropout_rate,dense_count,num_classe,learning_rate):\n",
    "    if (dense_count < 1):\n",
    "        return 'Favor inserir pelo menos um block denso'\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    neuron_count = 2048\n",
    "    for j in range(1,dense_count+1):\n",
    "        neuron_count = neuron_count/2\n",
    "        if(neuron_count == 1):\n",
    "            return \"Camadas densas demais, maximo 10 por favor\"\n",
    "    \n",
    "        if(j==1):\n",
    "            model.add(Dense(neuron_count, input_dim=3072, activation='relu'))\n",
    "        if(j==dense_count):\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "        else:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(neuron_count, activation='relu', kernel_constraint=maxnorm(3))) \n",
    "    \n",
    "    adam = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolhendo arquitetura da NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_epochs = 5\n",
    "fixed_batch_size = 32\n",
    "fixed_dropout_rate = 0.2\n",
    "fixed_lrate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_block_options = [3,4,5,6]\n",
    "\n",
    "\n",
    "max_accuracy = 0\n",
    "best_dense_block_size = 0\n",
    "\n",
    "X_train_flat = X_train.reshape(50000, 3072)\n",
    "X_test_flat = X_test.reshape(10000, 3072)\n",
    "\n",
    "\n",
    "\n",
    "for dense_block_num in dense_block_options:\n",
    "    model_NN = nn(fixed_dropout_rate,\n",
    "                  dense_block_num,\n",
    "                  num_classes,\n",
    "                  fixed_lrate)\n",
    "    \n",
    "    history_NN = model_NN.fit(X_train_flat, y_train, \n",
    "                                validation_data=(X_test_flat, y_test), \n",
    "                                epochs=fixed_epochs, \n",
    "                                batch_size=fixed_batch_size, \n",
    "                                verbose=1)\n",
    "    \n",
    "    if(history_NN.history[\"val_accuracy\"][-1] > max_accuracy):\n",
    "        max_accuracy = history_NN.history[\"val_accuracy\"][-1]\n",
    "        best_dense_block_size = dense_block_num\n",
    "        print(\"Trocou! Novo melhor numero de blocos densos eh\",best_dense_block_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escolhendo melhores hiperparametros para a NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate_options = [0.002,0.001]\n",
    "dropout_rate_options = [0.2,0.3]\n",
    "\n",
    "best_lrate = 0\n",
    "best_dropout_rate = 0\n",
    "\n",
    "max_accuracy = 0\n",
    "for lrate_num in lrate_options:\n",
    "    for dropout_rate_num in dropout_rate_options:\n",
    "        model_NN = nn(dropout_rate_num,best_dense_block_size,num_classes,lrate_num)\n",
    "        history_NN = model_NN.fit(X_train_flat, y_train, \n",
    "                                    validation_data=(X_test_flat, y_test), \n",
    "                                    epochs=fixed_epochs, \n",
    "                                    batch_size=fixed_batch_size, \n",
    "                                    verbose=1)\n",
    "        if(history_NN.history[\"val_accuracy\"][-1] > max_accuracy):\n",
    "            max_accuracy = history_NN.history[\"val_accuracy\"][-1]\n",
    "            \n",
    "            best_lrate = lrate_num\n",
    "            \n",
    "            best_dropout_rate = dropout_rate_num\n",
    "            print(\"Trocou! Novo melhor learning rate eh\",best_lrate)\n",
    "            print(\"Trocou! Novo melhor dropout rate eh \",best_dropout_rate)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando a melhor NN encontrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_NN_epoch = 5\n",
    "best_NN_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_NN_model = nn(best_dropout_rate,\n",
    "                     best_dense_block_size,\n",
    "                     num_classes,\n",
    "                     best_lrate)\n",
    "\n",
    "best_NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_NN_model.fit(X_train_flat, y_train, \n",
    "            validation_data=(X_test_flat, y_test), \n",
    "            epochs=best_NN_epoch, \n",
    "            batch_size= best_NN_batch_size)\n",
    "\n",
    "scores = best_NN_model.evaluate(X_test_flat, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de confusao da melhor NN encontrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_best_nn = best_NN_model.predict(X_test_flat)\n",
    "max_index_row_pred = np.argmax(pred_best_nn, axis=1)\n",
    "max_index_row_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "label_names = ['airplane','automobile','bird','cat',  'deer','dog','frog','horse','ship', 'truck']\n",
    "\n",
    "plot_confusion_matrix(max_index_row_test, max_index_row_pred, \n",
    "                      #classes=['0' ,'1','2','5', '6', '7', '4', '8', '3','9'],\n",
    "                      classes= label_names,\n",
    "                      title='NN confusion matrix',\n",
    "                     cmap = plt.cm.Oranges\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(dropout_rate, conv_block_count, dense_count, num_classes, learning_rate):\n",
    "    if conv_block_count < 1 or dense_count < 1:\n",
    "        return 'Favor inserir pelo menos um block denso e um block conv'\n",
    "    model = Sequential()\n",
    "    filter_count = 16\n",
    "    for i in range(1, conv_block_count+1):\n",
    "        filter_count = filter_count * 2\n",
    "        if (i ==1):\n",
    "            model.add(Conv2D(filter_count, (3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "            model.add(Dropout(dropout_rate)) \n",
    "            model.add(Conv2D(filter_count, (3, 3), activation='relu', padding='same'))\n",
    "            model.add(MaxPooling2D())\n",
    "        else:\n",
    "            model.add(Conv2D(filter_count, (3, 3), activation='relu', padding='same'))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(Conv2D(filter_count, (3, 3), activation='relu', padding='same'))\n",
    "            model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    neuron_count = 2048\n",
    "    for j in range(1,dense_count+1):\n",
    "        neuron_count = neuron_count/2\n",
    "        if(neuron_count == 1):\n",
    "            return \"Camadas densas demais, maximo 10 por favor\"\n",
    "        if (j==dense_count):\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "        else:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(neuron_count, activation='relu', kernel_constraint=maxnorm(3))) \n",
    "            \n",
    "            \n",
    "    adam = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolhendo a arquitetura da CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_epochs = 5\n",
    "fixed_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_block_options = [2,3]\n",
    "dense_block_options = [3,4]\n",
    "lrate = 0.001\n",
    "dropout_rate = 0.2\n",
    "max_accuracy = 0\n",
    "best_conv_block_size = 0\n",
    "best_dense_block_size = 0\n",
    "\n",
    "\n",
    "\n",
    "for conv_block_num in conv_block_options:\n",
    "    for dense_block_num in dense_block_options:\n",
    "        model_CNN = cnn(dropout_rate,conv_block_num,dense_block_num,num_classes,lrate)\n",
    "        history_CNN = model_CNN.fit(X_train, y_train, \n",
    "                                    validation_data=(X_test, y_test), \n",
    "                                    epochs=fixed_epochs, \n",
    "                                    batch_size=fixed_batch_size, \n",
    "                                    verbose=1)\n",
    "        if(history_CNN.history[\"val_accuracy\"][-1] > max_accuracy):\n",
    "            max_accuracy = history_CNN.history[\"val_accuracy\"][-1]\n",
    "            best_conv_block_size = conv_block_num\n",
    "            best_dense_block_size = dense_block_num\n",
    "            print(\"Trocou! Novo melhor numero de blocos conv eh\",best_conv_block_size)\n",
    "            print(\"Trocou! Novo melhor numero de blocos densos eh\",best_dense_block_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolhendo hiperparametros da CNN:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate_options = [0.002,0.001]\n",
    "dropout_rate_options = [0.3,0.2]\n",
    "\n",
    "best_lrate = 0\n",
    "best_dropout_rate =0\n",
    "\n",
    "max_accuracy = 0\n",
    "\n",
    "for lrate_num in lrate_options:\n",
    "    for dropout_rate_num in dropout_rate_options:\n",
    "        model_CNN = cnn(dropout_rate_num,best_conv_block_size,best_dense_block_size,num_classes,lrate_num)\n",
    "        history_CNN = model_CNN.fit(X_train, y_train, \n",
    "                                    validation_data=(X_test, y_test), \n",
    "                                    epochs=fixed_epochs, \n",
    "                                    batch_size=fixed_batch_size, \n",
    "                                    verbose=1)\n",
    "        if(history_CNN.history[\"val_accuracy\"][-1] > max_accuracy):\n",
    "            max_accuracy = history_CNN.history[\"val_accuracy\"][-1]\n",
    "            \n",
    "            best_lrate = lrate_num\n",
    "            \n",
    "            best_dropout_rate = dropout_rate_num\n",
    "            print(\"Trocou! Novo melhor learning rate eh\",best_lrate)\n",
    "            print(\"Trocou! Novo melhor dropout rate eh \",best_dropout_rate)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitando a melhor CNN encontrada utilizando gerador de dados augumentados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_epochs = 5\n",
    "final_model_batch_size = 32\n",
    "\n",
    "aug_train = datagen.flow(X_train, y_train, batch_size= final_model_batch_size)\n",
    "steps = int(X_train.shape[0]/final_model_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_CNN_model = cnn(best_dropout_rate,\n",
    "                     best_conv_block_size,\n",
    "                     best_dense_block_size,\n",
    "                     num_classes,\n",
    "                     best_lrate)\n",
    "\n",
    "best_CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_CNN_model.fit_generator(aug_train, \n",
    "                                       steps_per_epoch=steps,\n",
    "                                       epochs=final_model_epochs,\n",
    "                                       validation_data=(X_test, y_test))\n",
    "# Final evaluation of the model\n",
    "scores = best_CNN_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy da melhor CNN: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de confusao da melhor CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_best_cnn = best_CNN_model.predict(X_test)\n",
    "max_index_row_pred = np.argmax(pred_best_cnn, axis=1)\n",
    "max_index_row_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "label_names = ['airplane','automobile','bird','cat',  'deer','dog','frog','horse','ship', 'truck']\n",
    "\n",
    "plot_confusion_matrix(max_index_row_test, max_index_row_pred, \n",
    "                      #classes=['0' ,'1','2','5', '6', '7', '4', '8', '3','9'],\n",
    "                      classes= label_names,\n",
    "                      title='CNN confusion matrix',\n",
    "                     cmap = plt.cm.Oranges\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final: Comparando os modelos:\n",
    "LeNet\n",
    "\n",
    "  Acuracia final em treino:\n",
    "  \n",
    "  Acuracia final em teste:\n",
    "\n",
    "NN\n",
    "\n",
    "  Acuracia em treino:\n",
    "  \n",
    "  Acuracia em teste:\n",
    "\n",
    "CNN\n",
    "\n",
    "  Acuracia em treino:\n",
    "  \n",
    "  Acuracia em teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import tree\n",
    "\n",
    "HOME_PATH = '/homeLocal/gzuin/'\n",
    "MODELS_PATH = HOME_PATH + 'RootCauses-Behaviour/Models/'\n",
    "TABLES_PATH = HOME_PATH\n",
    "GRAPHS_PATH = HOME_PATH + 'RootCauses-Behaviour/Graphs/'\n",
    "IMAGES_PATH = HOME_PATH + 'RootCauses-Behaviour/Clusters/'\n",
    "\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 1\n",
    "\n",
    "\n",
    "def random_combinations(iterable, r, x, seed=10):\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    a = []\n",
    "    random.seed(seed)\n",
    "    for i in range(x):\n",
    "        indices = sorted(random.sample(range(n), r))\n",
    "        a.insert(len(a), tuple(pool[i] for i in indices))\n",
    "    return list(set(a))\n",
    "\n",
    "\n",
    "def gridSearch(df, comb, c):\n",
    "    bestParams = {'random_state': 20200225, 'criterion': 'gini', 'max_depth': None,\n",
    "                  'min_samples_split': 71, 'min_samples_leaf': 29, 'min_impurity_decrease': 0.0}\n",
    "    bestAUC = -1\n",
    "\n",
    "    mdrange = [None, 3, 5, 10]\n",
    "    criterions = ['gini', 'entropy']\n",
    "    mssrange = list(range(5, 101, 5))\n",
    "    mslrange = list(range(5, 51, 5))\n",
    "    midrange = [0.0, 0.01, 0.1]\n",
    "\n",
    "    combinations = len(mdrange)*len(mssrange)*len(mslrange)*len(midrange)\n",
    "\n",
    "    if os.path.isfile(MODELS_PATH + 'MultipleModels_DecisionTrees/' + 'suicidio-size%d-gridsearch.pkl' % c):\n",
    "        with open(MODELS_PATH + 'MultipleModels_DecisionTrees/' + 'suicidio-size%d-gridsearch.pkl' % c, 'rb') as pkldic:\n",
    "            bestParams = pickle.load(pkldic)\n",
    "            griddone[c] = combinations\n",
    "        return bestParams\n",
    "\n",
    "    tdone = 0.0\n",
    "    begin = time.time()\n",
    "    z = 0\n",
    "    for crit in criterions:\n",
    "        for md in mdrange:\n",
    "            for mss in mssrange:\n",
    "                for msl in mslrange:\n",
    "                    for mid in midrange:\n",
    "                        tdone += 1\n",
    "                        aucs = []\n",
    "                        # ff eh um modelo, comb eh uma lista de tuplas onde cada tupla é um modelo(conjunto de features) de mesmo tamanho\n",
    "                        for ff in comb:\n",
    "                            f = []\n",
    "                            for x in ff:  # transforma a tupla com o modelo em uma lista\n",
    "                                f.insert(len(f), x)\n",
    "                            auc, accmedia, preds, probs,f1,_ = select_features_platelabel(df, f,{'random_state': 1, 'max_depth': md, 'min_samples_split': mss, 'min_samples_leaf': msl, 'min_impurity_decrease': mid, 'criterion': crit}, nfolds=4,f1i=True)\n",
    "                            aucs.extend(auc)\n",
    "                            z += 1\n",
    "                            # print(z,len(comb))\n",
    "\n",
    "                        if np.mean(f1) > bestAUC:\n",
    "                            bestParams['max_depth'] = md\n",
    "                            bestParams['min_samples_split'] = mss\n",
    "                            bestParams['min_samples_leaf'] = msl\n",
    "                            bestParams['min_impurity_decrease'] = mid\n",
    "                            bestParams['criterion'] = crit\n",
    "                            bestAUC = np.mean(f1)\n",
    "\n",
    "                        if c != 0:\n",
    "                            now = time.time()\n",
    "                            elapsed = now-begin\n",
    "                            perinstance = float(elapsed)/float(tdone)\n",
    "                            predicted = perinstance * combinations\n",
    "                            griddone[c] += 1\n",
    "                            sys.stdout.write('GridSearch (size %02d) Progress: %.3f%% (%d/%d) [Elapsed: %ds | Predicted %ds | Avg: %ds]\\r' % (\n",
    "                                c, 100.0*tdone/combinations, tdone, combinations, elapsed, predicted, perinstance))\n",
    "                            sys.stdout.flush()\n",
    "\n",
    "    with open(MODELS_PATH + 'MultipleModels_DecisionTrees/' + 'suicidio-size%d-gridsearch.pkl' % c, 'wb') as pkldic:\n",
    "        pickle.dump(bestParams, pkldic)\n",
    "    return(bestParams)\n",
    "\n",
    "\n",
    "# Recebe varios modelos DE MESMO TAMANHO, cada uma é uma tupla em comb\n",
    "def eval_panel_platelabel(df, comb, c, exit_stat, exit_outp):\n",
    "\n",
    "    fpath = MODELS_PATH + 'MultipleModels_DecisionTrees/' + \\\n",
    "        'suicidio-size%d-preds.csv' % c\n",
    "\n",
    "    performed = []\n",
    "    if os.path.isfile(fpath) and os.path.getsize(fpath) > 0:\n",
    "        performed = list(pd.read_csv(\n",
    "            fpath, delimiter=';', header=0)['features'])\n",
    "        done[c] += len(performed)\n",
    "        global predone\n",
    "        predone += len(performed)\n",
    "\n",
    "        exit_outp.write('\\n')\n",
    "        exit_stat.write('\\n')\n",
    "    else:\n",
    "        exit_outp.write('features')\n",
    "        for i in range(len(df)):\n",
    "            exit_outp.write(';pred%d' % (i+1))\n",
    "        for i in range(len(df)):\n",
    "            exit_outp.write(';prob%d' % (i+1))\n",
    "        exit_outp.write('\\n')\n",
    "\n",
    "    params = gridSearch(df, comb[:max(50, int(0.001*float(len(comb))))], c)\n",
    "\n",
    "    ncomb = []\n",
    "    begin = time.time()\n",
    "    tdone = 0.0\n",
    "\n",
    "    for ff in comb:\n",
    "        f = []\n",
    "        for x in ff:  # transforma a tupla com o modelo em uma lista\n",
    "            f.insert(len(f), x)\n",
    "\n",
    "        if str(f) not in performed:\n",
    "            ncomb.append(ff)\n",
    "    comb = ncomb\n",
    "    res = []\n",
    "    # ff eh um modelo, comb eh uma lista de tuplas onde cada tupla é um modelo(conjunto de features) de mesmo tamanho\n",
    "    for ff in comb:\n",
    "        tdone += 1\n",
    "        now = time.time()\n",
    "        elapsed = now-begin\n",
    "        perinstance = float(elapsed)/float(tdone)\n",
    "        predicted = perinstance * len(comb)\n",
    "        sys.stdout.write('MM (size %02d) Progress: %.3f%% (%d/%d) [Elapsed: %ds | Predicted %ds | Avg: %ds]\\r' % (\n",
    "            c, 100.0*tdone/len(comb), tdone, len(comb), elapsed, predicted, perinstance))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        global s\n",
    "        if done[c] > s:\n",
    "            break\n",
    "\n",
    "        f = []\n",
    "        for x in ff:  # transforma a tupla com o modelo em uma lista\n",
    "            f.insert(len(f), x)\n",
    "        auc, accmedia, preds, probs, f1, f1w = select_features_platelabel(\n",
    "            df, f, params, nfolds=N_FOLDS, f1i=True)  # Chama a funcao central com apenas um modelo f\n",
    "        done[c] += 1\n",
    "        res.append(np.mean(auc))\n",
    "        exit_stat.write(\"%s;%f;%f;%f;%s;%s;%s;%s\\n\" %\n",
    "                        (str(f), np.mean(auc),np.mean(f1),np.mean(f1w),auc,f1,f1w,accmedia))\n",
    "\n",
    "        exit_outp.write(\"%s\" % str(f))\n",
    "        for p in preds:\n",
    "            exit_outp.write(';%d' % p)\n",
    "        for p in probs:\n",
    "            exit_outp.write(';%f' % p)\n",
    "        exit_outp.write('\\n')\n",
    "    return(res)\n",
    "\n",
    "\n",
    "def delete_last_lines(ifile):\n",
    "    with open(ifile, \"r+\", encoding=\"utf-8\") as file:\n",
    "\n",
    "        # Move the pointer (similar to a cursor in a text editor) to the end of the file\n",
    "        file.seek(0, os.SEEK_END)\n",
    "\n",
    "        # This code means the following code skips the very last character in the file -\n",
    "        # i.e. in the case the last line is null we delete the last line\n",
    "        # and the penultimate one\n",
    "        pos = file.tell() - 1\n",
    "\n",
    "        # Read each character in the file one at a time from the penultimate\n",
    "        # character going backwards, searching for a newline character\n",
    "        # If we find a new line, exit the search\n",
    "        while pos > 0 and file.read(1) != \"\\n\":\n",
    "            pos -= 1\n",
    "            file.seek(pos, os.SEEK_SET)\n",
    "\n",
    "        # So long as we're not at the start of the file, delete all the characters ahead\n",
    "        # of this position\n",
    "        if pos > 0:\n",
    "            file.seek(pos, os.SEEK_SET)\n",
    "            file.truncate()\n",
    "\n",
    "\n",
    "def select_features_platelabel(df, features, params, nfolds, f1i=False):  # Recebe UM modelo\n",
    "\n",
    "    X = df[features].values\n",
    "    y = df[label_column_name].values\n",
    "    predList = np.zeros(len(df))\n",
    "    probList = np.zeros(len(df))\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=1)\n",
    "    foldNum = 0\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    d = []\n",
    "    for (train, val) in cv.split(X, y):\n",
    "        #print(np.sum(y[train]),np.sum(y[val],len(y))\n",
    "        foldNum = foldNum + 1\n",
    "\n",
    "        # Modelo arvore\n",
    "        classifier = DecisionTreeClassifier(class_weight='balanced', max_depth=params['max_depth'], min_samples_leaf=params['min_samples_leaf'],min_samples_split=params['min_samples_split'], min_impurity_decrease=params['min_impurity_decrease'], criterion=params['criterion'])\n",
    "        classifier = classifier.fit(X[train], y[train])\n",
    "        probas_ = classifier.predict_proba(X[val])\n",
    "        probas = [probas_[x][0] for x in range(len(probas_))]\n",
    "\n",
    "        pred = classifier.predict(X[val])\n",
    "        area1 = roc_auc_score(y[val], probas_[:, 1])\n",
    "        area2 = accuracy_score(y[val], pred)  # guarda acuracia\n",
    "\n",
    "        #print('b',np.sum(y[val]),np.sum(pred),len(y[val]))\n",
    "        f1 = f1_score(y[val],pred, average='binary')\n",
    "        #print('w',np.sum(y[val]),np.sum(pred),len(y[val]))\n",
    "        f1w = f1_score(y[val],pred, average='weighted')\n",
    "\n",
    "        a.insert(len(a), area1)\n",
    "        b.insert(len(b), area2)\n",
    "        c.insert(len(c),f1)\n",
    "        d.insert(len(d),f1w)\n",
    "\n",
    "        for j in range(len(val)):\n",
    "            predList[val[j]] = pred[j]\n",
    "            probList[val[j]] = probas[j]\n",
    "\n",
    "    if f1i:\n",
    "        return a, np.mean(b), predList, probList,c,d\n",
    "    return a, np.mean(b), predList, probList\n",
    "\n",
    "\n",
    "df = pd.read_csv(TABLES_PATH + 'totalcx7.csv')\n",
    "df = df.dropna(axis=0)\n",
    "columns = list(df.columns)\n",
    "\n",
    "label_column_name = 'suicidio'\n",
    "unwanted_columns = [] + [label_column_name]\n",
    "features_columns = [\n",
    "    item for item in columns if item not in unwanted_columns]\n",
    "print(len(df), np.sum(df[label_column_name]))\n",
    "print(len(features_columns))\n",
    "\n",
    "\n",
    "print(np.sum(df['suicidio']),len(df))\n",
    "\n",
    "global done\n",
    "global griddone\n",
    "global predone\n",
    "global queue_finished\n",
    "queue_finished = 0\n",
    "predone = 0\n",
    "global s\n",
    "s = 10000\n",
    "totalmodels = 0\n",
    "combs = []\n",
    "done = []\n",
    "griddone = []\n",
    "\n",
    "print('Creating Feature Combinations')\n",
    "for c in range(0, 21):\n",
    "    # print('\\t Size:%d'%c)\n",
    "    if c == 0:\n",
    "        combs.append([])\n",
    "    else:\n",
    "        combs.append(list(set(random_combinations(features_columns, c, s))))\n",
    "    done.append(0)\n",
    "    griddone.append(0)\n",
    "    totalmodels += len(combs[-1])\n",
    "\n",
    "\n",
    "def run_mmpool(c):\n",
    "    sys.stdout.write(\"Starting MM size %d\\n\" % c)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    comb = combs[c]\n",
    "    exit1 = open(MODELS_PATH + 'MultipleModels_DecisionTrees/' +\n",
    "                 'suicidio-size%d-result.csv' % c, 'a+')\n",
    "    exit2 = open(MODELS_PATH + 'MultipleModels_DecisionTrees/' +\n",
    "                 'suicidio-size%d-preds.csv' % c, 'a+')\n",
    "\n",
    "    a = eval_panel_platelabel(df, comb, c, exit1, exit2)\n",
    "    global queue_finished\n",
    "    queue_finished += 1\n",
    "    exit1.close()\n",
    "    exit2.close()\n",
    "    return(a)\n",
    "\n",
    "\n",
    "if 1==1:\n",
    "    print('Creating Directories')\n",
    "    if (not os.path.isdir(MODELS_PATH + 'MultipleModels_DecisionTrees')):\n",
    "        os.mkdir(MODELS_PATH + 'MultipleModels_DecisionTrees')\n",
    "\n",
    "    for c in range(1, 21):\n",
    "        if os.path.isfile('MultipleModels_DecisionTrees/' + 'suicidio-size%d-result.csv' % c):\n",
    "            delete_last_lines('MultipleModels_DecisionTrees/' +\n",
    "                              'suicidio-size%d-result.csv' % c)\n",
    "            delete_last_lines('MultipleModels_DecisionTrees/' +\n",
    "                              'suicidio-size%d-preds.csv' % c)\n",
    "\n",
    "    pool = Pool(processes=10)\n",
    "    results = pool.map(run_mmpool, list(range(1, 21)))\n",
    "    pool.join()\n",
    "\n",
    "    for i in range(0, len(results)):\n",
    "        print(np.max(results[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

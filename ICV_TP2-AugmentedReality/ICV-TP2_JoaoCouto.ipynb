{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "whole-barrier",
   "metadata": {},
   "source": [
    "## João Marcos Machado Couto\n",
    "## Matricula: 2017014421\n",
    "### Link do vídeo: https://youtu.be/OsRTctj1IFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from OpenGL.GLU import *\n",
    "from OpenGL import *\n",
    "from OpenGL.GLUT import *\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from objloader import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "pikachuOn = True\n",
    "cubesOn = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-slope",
   "metadata": {},
   "source": [
    "# Calibração da Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-generic",
   "metadata": {},
   "source": [
    "Feita via \"Camera Calibration Toolbox for Matlab\" de Jean-Yves Bouguet (http://www.vision.caltech.edu/bouguetj/calib_doc/)\n",
    "\n",
    "Capturei frames 4 aleátorios do vídeo disponibilizado para fazer a calibração\n",
    "\n",
    "A seguir, os parametros obtidos e suas respectivas incertezas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-locking",
   "metadata": {},
   "source": [
    "Focal Length:          fc = [ 823.02785   857.55218 ] +/- [ 27.59011   25.29775 ]\n",
    "\n",
    "Principal point:       cc = [ 566.92369   396.42114 ] +/- [ 12.48074   22.10106 ]\n",
    "\n",
    "Skew:             alpha_c = [ 0.00000 ] +/- [ 0.00000  ]   => angle of pixel axes = 90.00000 +/- 0.00000 degrees\n",
    "\n",
    "Distortion:            kc = [ 0.09076   -0.21946   -0.00693   -0.00024  0.00000 ] +/- [ 0.07284   0.50725   0.00776   0.00641  0.00000 ]\n",
    "\n",
    "Pixel error:          err = [ 0.37915   0.42384 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-vegetation",
   "metadata": {},
   "source": [
    "Com essa saída definimos a matriz de parametros intrinsecos da câmera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsicMatrix = np.array([\n",
    "                            [823.02785, 0.0, 320], \n",
    "                            [0.0, 857.55218, 240], \n",
    "                            [0.0, 0.0, 1.0]\n",
    "                            ])\n",
    "cameraDistortion = [ 0.09076, -0.21946 ,-0.00693 ,-0.00024, 0.00000 ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-journalist",
   "metadata": {},
   "source": [
    "# Leitura de inputs (alvo+video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-rider",
   "metadata": {},
   "source": [
    "###  O Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-mapping",
   "metadata": {},
   "source": [
    "#####  leitura do vídeo de input via OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-circular",
   "metadata": {},
   "source": [
    "Ref: https://theailearner.com/2018/10/15/extracting-and-saving-video-frames-using-opencv-python/\n",
    "\n",
    "Ref2: https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputVideo = cv2.VideoCapture('entrada.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-leeds",
   "metadata": {},
   "source": [
    "##### Captura e decodificação de frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read retorna False, NONE quando não consegue ler mais conteúdo\n",
    "frames = []\n",
    "success,image = inputVideo.read()\n",
    "while success:\n",
    "    frames.append(image)\n",
    "    success, image = inputVideo.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-funds",
   "metadata": {},
   "source": [
    "### O alvo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-indonesian",
   "metadata": {},
   "source": [
    "##### Leitura e binarização do alvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "alvo0rot = cv2.imread('alvo.jpg', 0)\n",
    "_, alvo0rot = cv2.threshold(alvo0rot, 127, 255, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-ridge",
   "metadata": {},
   "source": [
    "##### Rotações do alvo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-embassy",
   "metadata": {},
   "source": [
    "Ref: https://www.geeksforgeeks.org/python-opencv-cv2-rotate-method/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "alvo1rot = cv2.rotate(alvo0rot, cv2.ROTATE_90_CLOCKWISE) \n",
    "alvo2rot = cv2.rotate(alvo1rot, cv2.ROTATE_90_CLOCKWISE) \n",
    "alvo3rot = cv2.rotate(alvo2rot, cv2.ROTATE_90_CLOCKWISE) \n",
    "alvos = [alvo0rot, alvo1rot,alvo2rot,alvo3rot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista auxiliar que permite acesso rápido às coordenadas dos 4 pontos de cada rotação do alvo\n",
    "pontosAlvo =[]\n",
    "pontosAlvo.append(np.float32([[-1, 1, 0], [-1, -1, 0], [1, -1, 0], [1, 1, 0]]))\n",
    "pontosAlvo.append(np.float32([[-1, -1, 0], [1, -1, 0], [1, 1, 0], [-1, 1, 0]]))\n",
    "pontosAlvo.append(np.float32([[1, -1, 0], [1, 1, 0], [-1, 1, 0], [-1, -1, 0]]))\n",
    "pontosAlvo.append(np.float32([[1, 1, 0], [-1, 1, 0], [-1, -1, 0], [1, -1, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-worship",
   "metadata": {},
   "source": [
    "# Determinando a posição e orientação do alvo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-assembly",
   "metadata": {},
   "source": [
    "### Extração de bordas e contornos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-bottle",
   "metadata": {},
   "source": [
    "Extração de bordas (B&W -> Binarização -> Bordas)\n",
    "\n",
    "Ref: https://docs.opencv.org/master/da/d22/tutorial_py_canny.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Um grayscale é criado antes de fazer a binarização\n",
    "def binarize(image):\n",
    "    grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binarizado = cv2.threshold(grayscale, 127, 255, cv2.THRESH_BINARY)\n",
    "    return binarizado\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dado um frame binarizado, extrai as bordas da imagem\n",
    "def extract_edges_given_binarized(binarizado):\n",
    "    imageEdges = cv2.Canny(binarizado, 100, 200)\n",
    "    return imageEdges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv2.TM_SQDIFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-binary",
   "metadata": {},
   "source": [
    "Extração de contornos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-leather",
   "metadata": {},
   "source": [
    "Ref: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_contours/py_contours_begin/py_contours_begin.html#contours-getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encontra os contornos de um frame dado o frame passado pelo detector de bordas canny\n",
    "def extract_contours_given_edges(imageEdges):\n",
    "    contorno, _  = cv2.findContours(imageEdges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-point",
   "metadata": {},
   "source": [
    "### Identificação de quadrilateros na imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-loading",
   "metadata": {},
   "source": [
    "Ref: https://stackoverflow.com/questions/55169645/square-detection-in-image/\n",
    "\n",
    "Ref2: https://stackoverflow.com/questions/61166180/detect-rectangles-in-opencv-4-2-0-using-python-3-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encontra os quadrilateros de um frame dado seus contornos\n",
    "#PS: \"square\" foi uma abuso da palavra pois na verdade neste ponto identifico quadrilateros\n",
    "def find_squares_given_contours(edgesContours):\n",
    "    squares = list()\n",
    "    for contour in edgesContours:\n",
    "        epsilon = 0.05*cv2.arcLength(contour,True)\n",
    "        polygon = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        if len(polygon) == 4 and cv2.isContourConvex(polygon):\n",
    "            squares.append(polygon)\n",
    "    return squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Só uma função auxiliar que retorna as coordenadas das quinas do alvo\n",
    "def template_corners_coords(template):\n",
    "    s = template.shape\n",
    "    return np.float32([[0,0], [0, s[0]], [s[1], s[0]], [s[1], 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-cambodia",
   "metadata": {},
   "source": [
    "### Homografia, perspective shift e identificação de quadilateros alvos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-appreciation",
   "metadata": {},
   "source": [
    "Faz a homografia e identificação dos quadrilateros que efetivamente dão match com alguma rotação do alvo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-bridge",
   "metadata": {},
   "source": [
    "#### Utilização do método CV_TM_SQDIFF: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-greensboro",
   "metadata": {},
   "source": [
    "#### $R(x,y) = \\sum(T(x',y') - I(x + x', y + y'))^2$  para calculo da diferença entre cada quadrilatero e as rotações do alvo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-ceramic",
   "metadata": {},
   "source": [
    "Retorna uma tupla de listas: ([contorno dos quadrados que deram match], [orientação deles])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-pulse",
   "metadata": {},
   "source": [
    "Experimentei boa parte dos métodos oferecidos pelo openCV para o calculo da diferença entre uma imagem e um template, a raíz da diferença quadrática provou-se eficaz na separação entre quadrilateros matching e não matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homography(corner_coords,quadrados, image):\n",
    "    quadrados_matched = [] #Armazena quadrilateros que deram match com alguma rotação do alvo\n",
    "    orientacao = [] #Armazena a orientação do alvo com o qual um dado quadrilatero deu match\n",
    "    for q in quadrados: \n",
    "        image_homography, _ = cv2.findHomography(np.float32(q), corner_coords, cv2.RANSAC)\n",
    "        warped = cv2.warpPerspective(image, image_homography, alvo0rot.shape) \n",
    "        leng = warped.size\n",
    "        \n",
    "        diffs = [0] * len(alvos) #Diff armazena a raiz da diferença quadratica entre um quadrilatero e cada rotação do alvo\n",
    "        for i,rot in enumerate(alvos):\n",
    "            delta = cv2.matchTemplate(warped,rot,0)\n",
    "            diffs[i] = np.sqrt(delta)\n",
    "\n",
    "        if min(diffs) < 30000 : #Se alguma das entradas no diff for menor que este threshold considera-se um match\n",
    "            quadrados_matched.append(q)\n",
    "            orientacao.append(diffs.index(min(diffs)))\n",
    "            \n",
    "\n",
    "    return (quadrados_matched, orientacao)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-payroll",
   "metadata": {},
   "source": [
    "# Efetivando identificação de alvos em todos os frames do vídeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-seating",
   "metadata": {},
   "source": [
    "Itera sobre todos os frames capturados do video e então os processa com as funções na ordem que vimos acima\n",
    "\n",
    "Binary -> Edges -> Contours -> \"Squares\" -> Homografia -> Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "cornerCoords = template_corners_coords(alvo0rot)\n",
    "cena = []\n",
    "for frame in frames:\n",
    "    binarizado = binarize(frame)\n",
    "    edges = extract_edges_given_binarized(binarizado)\n",
    "    contorno = extract_contours_given_edges(edges)\n",
    "    squares = find_squares_given_contours(contorno)\n",
    "    homo = homography(cornerCoords,squares, binarizado)\n",
    "    cena.append(homo)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-excitement",
   "metadata": {},
   "source": [
    "# Determinando os parâmetros extrínsecos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-teach",
   "metadata": {},
   "source": [
    "Ref: https://docs.opencv.org/master/d7/d53/tutorial_py_pose.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-forwarding",
   "metadata": {},
   "source": [
    "Faz a estimativa dos parametros extrinsecos\n",
    "\n",
    "Código quase identico ao explicitado em sala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose(polygon, direction):   \n",
    "\n",
    "    _, rot, trans = cv2.solvePnP(pontosAlvo[direction], np.float32(polygon), intrinsicMatrix, np.float32(cameraDistortion))\n",
    "    rodRot, _ = cv2.Rodrigues(rot)\n",
    "    \n",
    "    \n",
    "    matriz_pose = np.append(rodRot, trans, axis=1)\n",
    "    lastRow = [[0,0,0,1]]\n",
    "    matriz_pose = np.append(matriz_pose, lastRow, axis = 0)\n",
    "    \n",
    "       \n",
    "    matriz_pose[1, 0] =  matriz_pose[1, 0] * -1\n",
    "    matriz_pose[2, 0] = matriz_pose[2, 0] * -1 \n",
    "    matriz_pose[1, 1] = matriz_pose[1, 1] * -1\n",
    "    matriz_pose[2, 1] = matriz_pose[2, 1] * -1\n",
    "    matriz_pose[1, 2] = matriz_pose[1, 2] * -1\n",
    "    matriz_pose[2, 2] = matriz_pose[2, 2] * -1\n",
    "    matriz_pose[1, 3] = matriz_pose[1, 3] * -1\n",
    "    matriz_pose[2, 3] = matriz_pose[2, 3] * -1\n",
    "    \n",
    "    return np.transpose(matriz_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-channels",
   "metadata": {},
   "source": [
    "# Renderização via OpenGL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-weapon",
   "metadata": {},
   "source": [
    "Ref: https://www.youtube.com/watch?v=M4qFGp5muVg&feature=youtu.be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-concentrate",
   "metadata": {},
   "source": [
    "Função auxiliar para a obtenção de arestas de um cubo dado as cordenadas de seus vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def differ_counter(v1,v2):\n",
    "    count = 0\n",
    "    for i in range(len(v1)):\n",
    "        if v1[i] != v2[i]:\n",
    "            count = count + 1\n",
    "    return count\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-copying",
   "metadata": {},
   "source": [
    "### Renderização: cubo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(): \n",
    "    vertices=(\n",
    "        (1, -1, -1), (1, 1, -1), (-1, 1, -1),(-1, -1, -1),\n",
    "        (1, -1, 1), (1, 1, 1),(-1, -1, 1), (-1, 1, 1)\n",
    "    )\n",
    "    \n",
    "    arestas = []\n",
    "    #Em nosso cubo, vertices tem arestas entre si apenas se se diferem por apenas uma coordenada\n",
    "    #Essa sequencia de comandos constroi tupla de aresta apartir desta propriedade\n",
    "    for i,v1 in enumerate(vertices):\n",
    "        for j,v2 in enumerate(vertices):\n",
    "            c = differ_counter(v1,v2)\n",
    "            if (c ==1):\n",
    "                arestas.append(sorted(list([i,j])))\n",
    "    arestas = [tuple(x) for x in set(tuple(x) for x in arestas)]\n",
    "    arestas.remove((1,2)) \n",
    "    \n",
    "           \n",
    "    #Desenha a aresta indicativa da direção do cubo\n",
    "    glPushAttrib(GL_CURRENT_BIT)\n",
    "    glLineWidth(4)\n",
    "    glBegin(GL_LINES)\n",
    "    glColor3f(0., 0., 255/255)\n",
    "    glVertex3fv(vertices[1])\n",
    "    glVertex3fv(vertices[2])\n",
    "    \n",
    "    #Desenha o resto das arestas\n",
    "    glColor3f(255/255, 255/255, 255/255)\n",
    "    for aresta in arestas:\n",
    "        for vertice in aresta:\n",
    "            glVertex3fv(vertices[vertice])\n",
    "\n",
    "    glEnd()\n",
    "\n",
    "    glPopAttrib()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-criticism",
   "metadata": {},
   "source": [
    "### Renderização central: chama tanto pikachu quanto os cubos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render():\n",
    "    for p, d in zip([c[0] for c in cena][current_frame], [c[1] for c in cena][current_frame]):\n",
    "        glLoadMatrixf(pose(p, d)) \n",
    "        #Optei por renderizar os dois (pikachu e cubos) ao mesmo tempo pra facilitar a demonstração\n",
    "        \n",
    "        #Se for do interesse ver um deles de cada vez\n",
    "        ## basta setar cuberOn = Falsa ou pikachuOn=False na primeira celula\n",
    "        \n",
    "        if cubesOn: \n",
    "            cube()     \n",
    "        if pikachuOn:\n",
    "            glCallList(pikachu.gl_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-rugby",
   "metadata": {},
   "source": [
    "### Inicialização \"standard\" do openGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_open_gl():\n",
    "    glClearColor(0, 0, 0, 0) # Setando preto como cor de limpeza da tela\n",
    "    glClearDepth(1.0) \n",
    "    glEnable(GL_DEPTH_TEST) \n",
    "    glMatrixMode(GL_PROJECTION) \n",
    "    glLoadIdentity() \n",
    "    \n",
    "    fovy = 2 * np.arctan(0.5 * 480 / intrinsicMatrix[1, 1]) * 180 / np.pi\n",
    "    aspect = 640 * intrinsicMatrix[1, 1] / (480 * intrinsicMatrix[0, 0])\n",
    "    gluPerspective(fovy, aspect, 0.1, 100.0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-familiar",
   "metadata": {},
   "source": [
    "### Renderização: background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-length",
   "metadata": {},
   "source": [
    "Ref: https://www.youtube.com/watch?v=n4k7ANAFsIQ\n",
    "Ref2: https://www.youtube.com/watch?v=HOZA2ph4UuE&feature=youtu.be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def background(img):\n",
    "\n",
    "    (width, height) = (640,480)\n",
    "     \n",
    "    \n",
    "    # Ativação da textura\n",
    "    background_id = glGenTextures(1)\n",
    "    glBindTexture(GL_TEXTURE_2D, background_id)\n",
    "    \n",
    "    #Conversão a RGB utilizada pelo openGL e flip como visto em aula\n",
    "    background = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    background = cv2.flip(background, 0)\n",
    "    \n",
    "\n",
    "    # Criando a textura junto ao openGL\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, background)\n",
    "\n",
    "    # Desabilitando o glDepthMask para evitar que o fundo fique no topo\n",
    "    glDepthMask(GL_FALSE)\n",
    "    \n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    glPushMatrix()\n",
    "    glLoadIdentity()\n",
    "    gluOrtho2D(0, width, 0, height)\n",
    "    \n",
    "    #Ligando a textura ao background\n",
    "    glEnable(GL_TEXTURE_2D)\n",
    "    glBindTexture(GL_TEXTURE_2D, background_id)\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glPushMatrix()\n",
    "\n",
    "    #Desenha quadrilatero que ocupa a janela na integra\n",
    "    glBegin(GL_QUADS)\n",
    "    glTexCoord2f(0, 0); glVertex2f(0, 0)\n",
    "    glTexCoord2f(1, 0); glVertex2f(width, 0)\n",
    "    glTexCoord2f(1, 1); glVertex2f(width, height)\n",
    "    glTexCoord2f(0, 1); glVertex2f(0, height)\n",
    "    glEnd()\n",
    "    \n",
    "    glPopMatrix()\n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    glPopMatrix()\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    \n",
    "    # Desligamento da textura e flush\n",
    "    glBindTexture(GL_TEXTURE_2D, 0)\n",
    "    glDepthMask(GL_TRUE) #Impede que o fundo fique acima do objetos\n",
    "    glFlush()\n",
    " #Carrega o background. Recebe o frame da imagem (img)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-seeking",
   "metadata": {},
   "source": [
    "### Renderização: callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_frame = 0\n",
    "\n",
    "def display_callback(pikachu):\n",
    "    global current_frame #Utilizada para conseguir acessar globalmente qual é o index do proximo frame\n",
    "    glMatrixMode(GL_MODELVIEW) \n",
    "    glLoadIdentity() \n",
    "    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT) # Limpando buffers de cor/depth\n",
    "\n",
    "    background(frames[current_frame]) #Renderiza o background no frame atual\n",
    "    current_frame = (current_frame + 1) % len([c[0] for c in cena]) #Atualiza o index to frame atual para o proximo\n",
    "\n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    glLoadIdentity() \n",
    "\n",
    "    fovy = 2 * np.arctan(0.5 * 480 / intrinsicMatrix[1, 1]) * 180 / np.pi\n",
    "    aspect = 640 * intrinsicMatrix[1, 1] / (480 * intrinsicMatrix[0, 0])\n",
    "    gluPerspective(fovy, aspect, 0.1, 100.0)\n",
    "\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glLoadIdentity()\n",
    "    glEnable(GL_TEXTURE_2D)\n",
    "\n",
    "    render()\n",
    "    \n",
    "    glutSwapBuffers() #Renderiza na tela o que está atualmente em buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-climb",
   "metadata": {},
   "source": [
    "### Inicialização e loop main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "glutInit() #\n",
    "glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE) #Utilizaremos double buffer e espaço de cor RGB\n",
    "glutSetOption(GLUT_ACTION_ON_WINDOW_CLOSE, GLUT_ACTION_CONTINUE_EXECUTION) \n",
    "glutInitWindowSize(640, 480) \n",
    "glutCreateWindow(b'ICV TP2 - AR') \n",
    "\n",
    "init_open_gl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "pikachu = OBJ('Pikachu.obj', swapyz=True)\n",
    "display = lambda : display_callback(pikachu)\n",
    "glutDisplayFunc(display)\n",
    "glutIdleFunc(glutPostRedisplay) #Função a ser executada quando \"nada esta acontecendo\"\n",
    "\n",
    "glutMainLoop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-knowing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acute-seeker",
   "metadata": {},
   "source": [
    "# TP2 - Introdução à Computação Visual\n",
    "### Aluno: Evandro Lucas Figueiredo Teixeira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "higher-alexandria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'camera'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d7d948b2801b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mOpenGL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mOpenGL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLU\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcamera\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCamera\u001b[0m \u001b[0;31m# Biblioteca auxiliar para configurar o lookAt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoord\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoord\u001b[0m \u001b[0;31m# Biblioteca auxiliar para compactar a representação de coordenadas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mobjloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;31m# Biblioteca auxiliar para carregar um objeto mtl em opengl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'camera'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "from OpenGL.GL import *\n",
    "from OpenGL.GLU import *\n",
    "from camera import Camera # Biblioteca auxiliar para configurar o lookAt\n",
    "from coord import Coord # Biblioteca auxiliar para compactar a representação de coordenadas\n",
    "from objloader import * # Biblioteca auxiliar para carregar um objeto mtl em opengl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-trademark",
   "metadata": {},
   "source": [
    "## Instruções de execução\n",
    "\n",
    "É importante notar que este documento tem a finalidade acadêmica de demonstração dos métodos relevantes para a renderização da frame de vídeo, detecção dos alvos, aplicação das transformações do Pikachu. Duas bibliotecas feitas por mim, `camera.py` e `coord.py`, são importadas neste documento, pois são métodos relativos à abstração de coordenadas e posicionamento da câmera, o que não é o foco deste TP. O código fonte destes dois métodos poderia ser copiado e colado aqui, mas são muitas linhas, o que comprometeria a estética do documento. \n",
    "\n",
    "Além dos dois arquivos, inclusos no envio, é necessário que tanto o arquivo `alvo.jpg` quanto os arquivos contidos em `pikach_obj.zip` estejam presentes e descompactados no mesmo diretório deste ipynb. Além disso, o comando `jupyter notebook` ou `jupyter lab` deve ser executado neste diretório, caso contrário o ambiente não irá reconhecer os três arquivos importados (`camera.py`,`coord.py` e `objloader.py`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-schedule",
   "metadata": {},
   "source": [
    "## Buscando pelo alvo na imagem\n",
    "\n",
    "Abaixo, a classe `TargetSearcher` é responsável por realizar a busca do alvo na imagem. \n",
    "O processo de busca do alvo acontece em etapas: \n",
    "- Tanto o alvo como a imagem são convertidos em imagens binárias, isso é, onde os pixels são absolutamente pretos ou absolutamente brancos. Podemos fazer isso pois temos certeza de que o alvo que buscamos será preservado na imagem, pois ele é preto e branco. Isso também nos ajuda a ganhar desempenho. \n",
    "- À partir da imagem binária, procuramos por contornos. Isso é feito através dos métodos `cv2.Canny()` e `cv2.findContours()`. \n",
    "- Cada contorno será análisado. Se o contorno for um quadrilátero, realizaremos a transformação perspectiva sobre a imagem contida no quadrilátero de forma que ela seja transformada em um quadrado. Contornos que possuírem pontos próximos aos de contornos já classificados como alvos serão descartados, evitando duplicatas.\n",
    "- Este quadrado é então redimensionado e comparado ao alvo (que também é quadrado e binário). A similaridade é calculada através do método `cv2.matchTemplate()`. Caso o quadrilátero esteja em uma posição próxima à de outro quadrilátero encontrado em uma frame anterior, o threshhold da similaridade é reduzido. \n",
    "- Note que o quadrilátero será comparado em todas as suas 4 possíveis orientações, sendo a de maior similaridade a orientação selecionada. \n",
    "- Por fim, são retornados tuplas contendo os contornos e suas correspondentes orientações.\n",
    "\n",
    "Note que temos uma classe ao invés de um método. Fazemos isso para economizar recursos, persistindo na classe dados que não devem ser re-calculados, como o alvo (e suas transformações) e os últimos 100 pontos encontrados (para descontar no threshold).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetSearcher:\n",
    "    \n",
    "    def __init__(self, target_filename):\n",
    "        \n",
    "        self.target_already_adjusted = False\n",
    "        self.target_filename = target_filename\n",
    "        self.saved_warps = 0\n",
    "\n",
    "    def setup_target(self,img_width,img_height):\n",
    "\n",
    "        max_dim = max(img_width,img_height)\n",
    "\n",
    "        # Target image dimensions\n",
    "        self.target_size = round((max_dim / 6)/10)*5\n",
    "        \n",
    "        # Parsing target image : loading, resizing it and converting it to binary/black and white\n",
    "        self.target = cv2.imread(self.target_filename)\n",
    "        self.target = cv2.cvtColor(self.target, cv2.COLOR_BGR2GRAY)\n",
    "        self.target = cv2.resize(self.target, (self.target_size, self.target_size), interpolation = cv2.INTER_AREA)\n",
    "        _, self.target = cv2.threshold(self.target, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        self.last_points=[]\n",
    "        self.target_already_adjusted = True\n",
    "\n",
    "    def quads_are_similar(self,quad1,quad2,threshold):\n",
    "            for c1 in quad1: # mc is a coordinate\n",
    "                for c2 in quad2: # cn is also a coordinate\n",
    "                    if (abs(c2[0] - c1[0]) < threshold) or (abs(c2[1] - c1[1]) < threshold):\n",
    "                        return True\n",
    "    \n",
    "    def search(self, frame):\n",
    "\n",
    "        ori = [[0,1,2,3],[3,0,1,2],[2,3,0,1],[1,2,3,0]]\n",
    "\n",
    "        # Parsing frame : Converting to black and white so we can detect the edges faster\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        _, img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        if not self.target_already_adjusted:\n",
    "            self.setup_target(img.shape[:2][1],img.shape[:2][0])\n",
    "\n",
    "        # Getting the edged version of the frame after applying the Canny algorithm \n",
    "        edged = cv2.Canny(img, 100, 200)\n",
    "        # Getting the countours. Each countor is a list of touples defining the boundary points.\n",
    "        countours, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        matches = []\n",
    "        for c in countours:\n",
    "            # Finding the aproximated shape of this countour\n",
    "            # We'll use a constant value for epsilon since it is to costly to run\n",
    "            approx = cv2.approxPolyDP(c, 15, True)\n",
    "            # If the lenght of the aproximated shape is 4, it means that we' may have found a quadrilateral, \n",
    "            # Which is ideal for the situation\n",
    "            \n",
    "            if len(approx) == 4:\n",
    "                approx = approx[0:4]\n",
    "                target_vertices = np.array([[0, 0], [0, self.target_size], [self.target_size, self.target_size], [self.target_size, 0]], dtype=\"float32\")\n",
    "\n",
    "                # Transforming the shape in a list of corner coordinates, since it is a quadrilateral\n",
    "                corners = np.around(np.array(approx.reshape(4,2), dtype=\"float32\"),1)\n",
    "\n",
    "                # Discarding duplicates\n",
    "                found_similar_coord = False\n",
    "                for m in matches: # m[0] is a list of of corner coordinates\n",
    "                    if self.quads_are_similar(corners,m[0],5):\n",
    "                        found_similar_coord = True\n",
    "                        break\n",
    "                if found_similar_coord:\n",
    "                    continue\n",
    "                \n",
    "                # For each possible rotation we'll see if it matches, so : \n",
    "                # Initializing auxiliar variables\n",
    "                found_rot = False\n",
    "                best_ori = None\n",
    "                best_avg = None\n",
    "                best_corners = None\n",
    "                last_best_simm = 0\n",
    "                for i in range(4):\n",
    "                    # We get the perspectiveTranformation matrix, with: \n",
    "                    #   corners being the \"source\" points. \n",
    "                    #   target_vertices being the mapping of the corners to the new image\n",
    "                    # After warping it, everything inside the image wil be \"stretched\" to fit \n",
    "                    # into the box defined by target_vertices\n",
    "                    # It may as well rotate it, depending on the order of those vertices\n",
    "                    tvo = target_vertices.copy()\n",
    "                    matrix = cv2.getPerspectiveTransform(corners, tvo[ori[i]])\n",
    "                    warped = cv2.warpPerspective(img, matrix, (self.target_size, self.target_size))\n",
    "                    # With this, we already have an image that we can compare to our target\n",
    "                    # We'll make it binary (black and white, not grayscale) : \n",
    "                    _, bw_img = cv2.threshold(warped, 128, 255, cv2.THRESH_BINARY)\n",
    "                    # Now we compare it to the rotated target\n",
    "                    simm = round(float(cv2.matchTemplate(bw_img,self.target,cv2.TM_CCOEFF_NORMED)[0][0]),4)\n",
    "\n",
    "                    threshhold = 0.6\n",
    "                    # We them will favor more those target who were found in a \n",
    "                    # similar position to previous targets\n",
    "                    col_sum = np.sum(corners, axis=0)\n",
    "                    avg = np.array([col_sum[0]/4,col_sum[1]/4])\n",
    "                    tolerance = 0\n",
    "                    if simm < threshhold and tolerance > 0:\n",
    "                        for lp in self.last_points:\n",
    "                            dist = abs(np.linalg.norm(avg-lp))\n",
    "                            if dist < 10: \n",
    "                                tolerance = threshhold/1.9\n",
    "                                break\n",
    "    \n",
    "                    if (simm >= (threshhold - tolerance)):\n",
    "                        if simm > last_best_simm:\n",
    "                            best_corners = corners\n",
    "                            best_ori = ori[i].copy()\n",
    "                            best_avg = avg\n",
    "                            found_rot = True\n",
    "                            last_best_simm = simm\n",
    "\n",
    "                if found_rot:\n",
    "                    matches.append((best_corners, list(best_ori)))\n",
    "                    if len(self.last_points) > 100 :\n",
    "                        self.last_points.pop(-1)\n",
    "                    self.last_points.insert(random.randint(0,len(self.last_points)),best_avg)\n",
    "        \n",
    "        # If no target match was found, we decrease one point from the memory\n",
    "        # We do this to avoid persisting old positions and alowing a threshold discount for non-target objects \n",
    "        if len(matches) == 0 :\n",
    "            if len(self.last_points) > 0 :\n",
    "                self.last_points.pop(-1)\n",
    "\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-mercy",
   "metadata": {},
   "source": [
    "Abaixo extraímos a matriz de rotação/translação correspondente ao alvo encontrado na imagem, que se encontra como um quadrilátero. O OpenCV já nos fornece todos os métodos que precisamos para gerar a matriz, o importante é repassarmos ao método os parâmetros intrínsecos da image, como a matrix da câmera e os coeficientes de distorção. Também utilizamos o vetor de orientação para rotacionar o modelo a ser transformado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(rect, index, camera_matrix, camera_dist):\n",
    "    # We'll change the index since the frame is rotate in 90 degrees\n",
    "    corners = np.array(rect, dtype=\"float32\")\n",
    "    orientation = np.array([[-1, -1, 1], [ 1, -1, 1], [ 1,  1, 1], [-1,  1, 1]], dtype=\"float32\")[index]\n",
    "    _, rotation_vector, transf_vector = cv2.solvePnP(orientation, corners, camera_matrix, camera_dist)\n",
    "    # Convertendo o vetor de rotação em uma matriz\n",
    "    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "    final_mat = np.array([\n",
    "        [ rotation_matrix[0][0],  rotation_matrix[0][1],  rotation_matrix[0][2],  transf_vector[0]], \n",
    "        [-rotation_matrix[1][0], -rotation_matrix[1][1], -rotation_matrix[1][2], -transf_vector[1]], \n",
    "        [-rotation_matrix[2][0], -rotation_matrix[2][1], -rotation_matrix[2][2], -transf_vector[2]],  \n",
    "        [                   0.0,                    0.0,                    0.0,               1.0]]\n",
    "    )\n",
    "    return np.transpose(final_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-murder",
   "metadata": {},
   "source": [
    "Exibir o vídeo em openGL é bem simples. Uma vez extraída a imagem (frame) do vídeo, aplicaremos esta imagem a um retângulo de mesma dimensão como uma textura. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rect_with_tex(texture, scale = 1):\n",
    "    texture_string = pygame.image.tostring(texture, \"RGBA\", 1)\n",
    "    glEnable(GL_TEXTURE_2D)\n",
    "    glBindTexture(GL_TEXTURE_2D, glGenTextures(1))\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, texture.get_width(), texture.get_height(), 0, GL_RGBA, GL_UNSIGNED_BYTE, texture_string)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S    , GL_REPEAT)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T    , GL_REPEAT)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST)\n",
    "    glBegin(GL_QUADS)\n",
    "    glTexCoord2f(0.0, 0.0); glVertex3f( 0, 0, 0)\n",
    "    glTexCoord2f(1.0, 0.0); glVertex3f( texture.get_width()*scale, 0, 0)\n",
    "    glTexCoord2f(1.0, 1.0); glVertex3f( texture.get_width()*scale, texture.get_height()*scale, 0)\n",
    "    glTexCoord2f(0.0, 1.0); glVertex3f( 0, texture.get_height()*scale, 0)\n",
    "    glEnd()\n",
    "    glDisable(GL_TEXTURE_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-heaven",
   "metadata": {},
   "source": [
    "Por motivos de desenvolvimento, optei por trabalhar com coordenadas reduzidas no opengl. O método abaixo converte um par de coordenadas do vídeo (pixels) em posições no plano de exibição das frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_to_video_coord(coord, width, height,scale = 1):\n",
    "    x = coord.x * scale\n",
    "    y = ((coord.y * (-1)) + height) * scale\n",
    "    z = coord.z\n",
    "    return Coord(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-motivation",
   "metadata": {},
   "source": [
    "Abaixo, temos a exibição do pikachu e do cubo. Ambos os métodos utilizam uma matriz de transformação `m`, responsável por transladar e transformar o objeto a ser desenhado. O método que exibe o pikachu recebe um parâmetro `obj`, que corresponde ao modelo do pikachu já carregado na memória. Fazemos isso para evitar de carregar o mesmo exato modelo em cada frame, o que é importantíssimo para se ter um desempenho decente. \n",
    "Também é importante notar que o DEPTH_TEST é manipulado durante a gerações destes objetos, de forma que: \n",
    "- O objeto sempre se mantenha na frente do frame do vídeo\n",
    "- Os polígonos do objeto não sejam exibidos com a profundidade invertida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pikachu(obj,m):\n",
    "    glDepthMask(GL_TRUE)\n",
    "    glEnable(GL_DEPTH_TEST)\n",
    "    glLoadMatrixd(m)\n",
    "    glScalef(0.7,0.7,0.7)\n",
    "    glCallList(obj.gl_list)\n",
    "    glDisable(GL_DEPTH_TEST)\n",
    "    \n",
    "def show_cube(m):\n",
    "    glDepthMask(GL_TRUE)\n",
    "    glEnable(GL_DEPTH_TEST)\n",
    "    glLoadMatrixd(m)\n",
    "    vertices = [(1, -1, 0),(1, 1, 0),(-1, 1, 0),(-1, -1, 0),(1, -1, 2),(1, 1, 2),(-1, -1, 2),(-1, 1, 2)]\n",
    "    edges = [(0,1),(0,3),(0,4),(2,1),(2,3),(2,7),(6,3),(6,4),(6,7),(5,1),(5,4),(5,7)]\n",
    "    glBegin(GL_LINES)\n",
    "    glColor3f(1, 0, 0)\n",
    "    for vs in edges:\n",
    "        for v in vs:\n",
    "            glVertex3fv(vertices[v])\n",
    "    glColor3f(1, 1, 1)\n",
    "    glEnd()\n",
    "    glDisable(GL_DEPTH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-privilege",
   "metadata": {},
   "source": [
    "O método principal consiste em vários estágios: \n",
    "- Configuramos dados relativos à janela de exibição do PyGame/OpenGL, como escala e dimensões\n",
    "- Configuramos a projeção e o FOV\n",
    "- Salvamos os parâmetros intrínsecos da câmera, extraídos através do calib_gui, via Matlab. \n",
    "- Posicionamos a câmera do OpenGL logo acima do ponto médio da frame, ajustado para a janela de exibição\n",
    "- Inicializamos o vídeo, o objeto do Pikachu, e o buscador do alvo.\n",
    "- No loop principal: \n",
    "    - Reposicionamos a câmera para observar o cenário logo acima do vídeo\n",
    "    - Reproduzimos uma frame do vídeo\n",
    "    - Extraímos os alvos encontrados utilizando o método `search` visto acima\n",
    "    - Se não encontrarmos alvo algum, repetimos o último alvo encontrado por até 5 frames (reduz o efeito do modelo \"piscando\")\n",
    "    - Exibimos os modelos (cubo ou pikachu) de acordo com as coordenadas do alvo e com os parâmetros intrínsecos da câmera\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(obj_name):\n",
    "\n",
    "    video_path = \"entrada.mp4\"\n",
    "    # Setting up the window dimensions according to video\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    hasFrame, frame = video.read()\n",
    "    video_h,video_w = frame.shape[:2]\n",
    "    video_scale_in_world =  0.01\n",
    "\n",
    "    screen_scale = 1.3\n",
    "    display = (math.ceil(video_w*screen_scale),math.ceil(video_h*screen_scale))\n",
    "    \n",
    "    print (\"Display: \" + str(display))\n",
    "\n",
    "    pygame.init()\n",
    "    pygame.display.set_mode(display,DOUBLEBUF|OPENGL)\n",
    "    pygame.mouse.set_visible(True)\n",
    "    glEnable(GL_DEPTH_TEST)\n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    gluPerspective(47, display[0]/display[1], 0.1, 100)\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "\n",
    "    # Camera intrinsic parameters\n",
    "    fx, fy = 509.40514, 510.89524\n",
    "    cx, cy = 320.04523, 250.21829\n",
    "    camera_matrix = np.array([\n",
    "        [fx, 0, cx], \n",
    "        [0, fy, cy], \n",
    "        [0,  0,  1]\n",
    "    ])\n",
    "    camera_dist = np.array([0.01027, 0.31924, -0.00040, 0.00033, 0.00000 ])\n",
    "\n",
    "\n",
    "    cam_x = (video_w/2)*video_scale_in_world\n",
    "    cam_y = (video_h/2)*video_scale_in_world\n",
    "    camera = Camera( (cam_x,cam_y,(5.5/640)*max(video_h,video_w)) , (cam_x,cam_y+0.001,0) , (0,0,1))\n",
    "    camera.lock()\n",
    "    \n",
    "    if obj_name == \"pikachu\":\n",
    "        pikachu_obj = OBJ(\"Pikachu.obj\", swapyz=True)\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    target_searcher = TargetSearcher(\"alvo.jpg\")\n",
    "\n",
    "    last_matches = []\n",
    "    last_matches_count = 0\n",
    "    last_matches_max = 5\n",
    "\n",
    "    keepRunning = True\n",
    "    while keepRunning:\n",
    "\n",
    "        pygame.display.flip()\n",
    "        pygame.time.wait(1)\n",
    "        pygame.event.pump()\n",
    "\n",
    "        glMatrixMode(GL_MODELVIEW)\n",
    "        glPushMatrix()\n",
    "        \n",
    "        # Background Color is set to black\n",
    "        glClearColor(0,0,0,0.1)\n",
    "        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n",
    "\n",
    "        # Camera control\n",
    "        camera.control(speed = 0.2)\n",
    "\n",
    "        # Reading one frame for the video\n",
    "        # If the video ends, we'll play it again\n",
    "        hasFrame, frame = video.read()\n",
    "        if not hasFrame: \n",
    "            video = cv2.VideoCapture(video_path)\n",
    "            hasFrame, frame = video.read()\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        cv2_image_to_pygame = pygame.image.frombuffer(frame.tostring(), frame.shape[1::-1],\"RGB\")\n",
    "        draw_rect_with_tex(cv2_image_to_pygame, scale = video_scale_in_world)\n",
    "    \n",
    "        matches = target_searcher.search(frame)\n",
    "\n",
    "        # Re-using previous matches if no match is found\n",
    "        # Reduces flickering\n",
    "        if len(matches) == 0 and last_matches_count > 0:\n",
    "            matches = last_matches\n",
    "            last_matches_count-=1\n",
    "        else:\n",
    "            last_matches = matches\n",
    "            last_matches_count=last_matches_max\n",
    "\n",
    "        # Drawing a pikachu for each match found\n",
    "        for m in matches:\n",
    "            corners = m[0]\n",
    "            orientation = m[1]\n",
    "            matrix = get_matrix(corners, orientation, camera_matrix, camera_dist)\n",
    "            if obj_name == \"pikachu\":\n",
    "                show_pikachu(pikachu_obj,matrix)\n",
    "            else:\n",
    "                show_cube(matrix)\n",
    "        \n",
    "        glPopMatrix()\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                keepRunning = False\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-gasoline",
   "metadata": {},
   "source": [
    "O código abaixo irá renderizar o Pikachu sobre os alvos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  main(\"pikachu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-brand",
   "metadata": {},
   "source": [
    "O código abaixo irá renderizar o cubo sobre os alvos (está comentado para que seja possível executar todo o notebook de uma vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"cube\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
